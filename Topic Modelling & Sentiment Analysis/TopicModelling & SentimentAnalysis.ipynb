{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDwep1K8Erxl"
   },
   "source": [
    "<h2>PROJECT: Twitter Topic Modelling and Sentiment Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re,json, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85WxmGNGDcBY"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/katenjoki/Twitter-Data-Analysis/main/data/covid19.json'\n",
    "df = pd.read_json(url,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique languages check?\n",
    "df['lang'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Data**\n",
    "\n",
    "* Drop null values\n",
    "* make all words lower_case\n",
    "* remove punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pd.DataFrame(columns=['original_text'])\n",
    "tweets['original_text']=df['text']\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "tweets.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking random original tweet to see if the tweets are standardised\n",
    "print('Original text\\n',tweets.at[55,'original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text']=tweets['original_text'].str.lower()\n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to drop punctuation marks\n",
    "\n",
    "import string\n",
    "def clean_text_column(tweet):\n",
    "    for punctuation in string.punctuation:\n",
    "        tweet = tweet.replace(punctuation,\" \")\n",
    "    return tweet\n",
    "\n",
    "tweets['clean_text']=tweets['clean_text'].apply(clean_text_column)\n",
    "\n",
    "print('Clean tweet: \\n',tweets.at[33,'clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets.drop('original_text',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Visualization using WordCloud</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator,STOPWORDS\n",
    "#STOPWORDS are words that don't contain enough significance\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(WordCloud(width=800,height=300,stopwords=STOPWORDS).generate(' '.join(tweets.clean_text .values)))\n",
    "plt.axis('off')\n",
    "plt.title('Most Frequent Words In Our Tweets',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Top 10 words to appear in Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "results = Counter()\n",
    "tweets['clean_text'].str.lower().str.split().apply(results.update)\n",
    "results =  pd.DataFrame.from_dict(results,orient='index',columns=['count'])\n",
    "results=results.sort_values(by='count',ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising Top 10 words in tweets\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "results[:10].plot.barh(colormap='Spectral')\n",
    "plt.title('Top 10 words in tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Topic Modelling </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary, Corpora, LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [tweet for tweet in tweets['clean_text']]\n",
    "words = [sent.split() for sent in sentences]\n",
    "\n",
    "print('Sentence\\n',sentences[:1],'\\n')#shows the first sentence\n",
    "print('The words split from the first sentence\\n',words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "#Create dictionary which contains Id and word \n",
    "word_id = corpora.Dictionary(words)\n",
    "for k, v in word_id.items():\n",
    "    print(k,\"........\",v)\n",
    "    \n",
    "tweet_list = [word_id.doc2bow(tweet) for tweet in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet_list[:1])\n",
    "\n",
    "id_words = [[(word_id[id],count) for id, count in tweet]for tweet in tweet_list]\n",
    "print('\\n First document: \\n',sentences[:1])#print actual words\n",
    "\n",
    "print('\\n The actual words in the first document \\n',id_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the above code**\n",
    "\n",
    "* shows that the word with ID=0 appears once,ID=11 appears thrice... in the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Fitting the LDA model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=tweet_list,id2word=word_id,num_topics=3,alpha='auto',per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.print_topics())\n",
    "doc_model= lda[tweet_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Interpretation of the LDA model results***\n",
    "\n",
    "* Topic 0 is represented by 0.036*\"covaxin\" + 0.028*\"vaccines\" + 0.027*\"capacity\" + 0.024*\"hospital\" + 0.023*\"dose2\" + 0.021*\"18\" + 0.021*\"age\" + 0.020*\"pin\" + 0.020*\"limit\" + 0.020*\"min\"\n",
    "* Meaning the top 10 keywords for topic 0 are covaxin, vaccines,capacity,hospital,dose2,18, age, pin,limit,min\n",
    "*  Where the weight of covaxin in topic 0 is 0.036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Visualising the Topics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "visuals = pyLDAvis.gensim_models.prepare(lda,tweet_list, word_id)\n",
    "visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>\n",
    "\n",
    "* Import TextBlob to determine polarity of the clean text\n",
    "* Create new column score\n",
    "* Visualise score column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "cleanTweet = pd.DataFrame(columns=['clean_text','polarity'])\n",
    "cleanTweet['clean_text'] = tweets['clean_text']\n",
    "\n",
    "tweet_blob = [TextBlob(tweet) for tweet in cleanTweet['clean_text']]\n",
    "cleanTweet['polarity'] = [b.sentiment.polarity for b in tweet_blob]\n",
    "cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity=[]\n",
    "def text_category(pol):\n",
    "    for p in pol:\n",
    "        if p > 0:\n",
    "            score = 'positive'\n",
    "        elif p == 0:\n",
    "            score = 'neutral'\n",
    "        else:\n",
    "            score = 'negative'\n",
    "        polarity.append(score)\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweet['score']=text_category(cleanTweet['polarity'])\n",
    "cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Piechart visualisation\n",
    "plt.pie(cleanTweet['score'].value_counts(),labels=cleanTweet['score'].unique(),autopct='%1.2f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barchart Visualisation\n",
    "sns.barplot(x=cleanTweet['score'].unique(),y=cleanTweet['score'].value_counts(),data=cleanTweet,palette='Accent')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building the Classification Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweet = cleanTweet[cleanTweet['score']!='neutral']\n",
    "cleanTweet.reset_index()\n",
    "cleanTweet.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweet['scoremap']=\"\"\n",
    "cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleanTweet['scoremap'] = cleanTweet['score'].apply(lambda x:1 if x=='positive' else 0)\n",
    "cleanTweet.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tweet_train, tweet_test = train_test_split(cleanTweet, test_size=0.3)\n",
    "tweet_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigram(3,3)\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "trigram_vectorizer.fit(tweet_train['clean_text'].values)\n",
    "\n",
    "X_train_trigram = trigram_vectorizer.transform(tweet_train['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tf_idf_transformer = TfidfTransformer()\n",
    "trigram_tf_idf_transformer.fit(X_train_trigram)\n",
    "\n",
    "X_train_trigram_tf_idf = trigram_tf_idf_transformer.transform(X_train_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train SDGClassifier </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tweet_train['scoremap'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def SDG_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, stratify=y)\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDG_scores(X_train_trigram, y_train, 'Trigram Counts')\n",
    "SDG_scores(X_train_trigram_tf_idf, y_train, 'Trigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validation score of 0.95 is pretty good\n",
    "* Next we test the model using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = trigram_vectorizer.transform(tweet_test['clean_text'].values)\n",
    "X_test = trigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = tweet_test['scoremap'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = SDG_scores(X_test, y_test,'Test Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The test accuracy is 94% "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenge_ Day2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
